{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description='''\n",
    "Job Description: Senior Backend Engineer\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "Design, develop, and maintain robust and scalable backend systems.\n",
    "Collaborate with frontend and mobile teams to build seamless user experiences.\n",
    "Optimize database performance and write efficient SQL queries.\n",
    "Implement robust security measures to protect sensitive data.\n",
    "Mentor junior engineers and foster a culture of continuous learning.\n",
    "Required Skills:\n",
    "\n",
    "Strong proficiency in backend programming languages (e.g., Python, Node.js, Ruby on Rails, Java).\n",
    "Experience with database technologies (e.g., PostgreSQL, MySQL, MongoDB).\n",
    "Solid understanding of RESTful API design and development.\n",
    "Knowledge of cloud platforms (e.g., AWS, GCP, Azure).\n",
    "Experience with containerization technologies (e.g., Docker, Kubernetes).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "interviewee_responce='''\n",
    "I've been passionate about backend development for 3 years, and I'm excited to apply my skills to challenging projects.\n",
    "At my previous role at egy_tech, I was responsible for building a scalable API that handled 100 requests per second.\n",
    "I utilized [Specific technologies, e.g., Python, Flask, PostgreSQL] to optimize performance and ensure reliability.\n",
    "\n",
    "I'm particularly interested in your company's focus on [database, data privacy, machine learning, Azuru].\n",
    "I've been exploring Node.js and believe it could be a valuable asset to your team.\n",
    "I'm eager to contribute to innovative projects and learn from experienced engineers.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preproessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Mohamed\n",
      "[nltk_data]     Walid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mohamed\n",
      "[nltk_data]     Walid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Mohamed\n",
      "[nltk_data]     Walid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "translated_table = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV  # Adverb\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r'\\d+', '', text)       # Remove numbers\n",
    "    text = text.translate(translated_table)\n",
    "\n",
    "    text_tokens = word_tokenize(text)\n",
    "\n",
    "    filtered_words=[word for word in text_tokens if word not in stop_words ]\n",
    "    # lemmatization => transforming words to their base or dictionary form\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "    lemma_words = []\n",
    "    for word in filtered_words:\n",
    "        pos_tag = nltk.pos_tag([word])[0][1]  # Get POS tag for each word\n",
    "        wordnet_pos = get_wordnet_pos(pos_tag)  # Map POS to WordNet POS\n",
    "        lemma_word = lemmatizer.lemmatize(word, pos=wordnet_pos)  # Lemmatize using WordNet POS\n",
    "        lemma_words.append(lemma_word)\n",
    "\n",
    "    processed_text = ' '.join(lemma_words)\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed job description : job description senior backend engineer responsibility design develop maintain robust scalable backend system collaborate frontend mobile team build seamless user experience optimize database performance write efficient sql query implement robust security measure protect sensitive data mentor junior engineer foster culture continuous learn require skill strong proficiency backend program language eg python nodejs ruby rail java experience database technology eg postgresql mysql mongodb solid understand restful api design development knowledge cloud platform eg aws gcp azure experience containerization technology eg docker kubernetes\n"
     ]
    }
   ],
   "source": [
    "preprocessed_job_description = preprocess_text(job_description)\n",
    "print(f\"Preprocessed job description : {preprocessed_job_description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed interviewee responce : ive passionate backend development year im excite apply skill challenge project previous role egytech responsible building scalable api handle request per second utilized specific technology eg python flask postgresql optimize performance ensure reliability im particularly interested company focus database data privacy machine learn azuru ive explore nodejs believe could valuable asset team im eager contribute innovative project learn experienced engineer\n"
     ]
    }
   ],
   "source": [
    "preprocessed_interviewee_responce= preprocess_text(interviewee_responce)\n",
    "print(f\"Preprocessed interviewee responce : {preprocessed_interviewee_responce}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract important keywords From Job Description and Interviewee responce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pip install keybert spacy nltk\n",
    "##### python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_relevant_keywords(text, nlp=None, min_word_length=2, max_keywords=15):\n",
    "    \"\"\"\n",
    "    Extract relevant keywords with robust filtering and customization options.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text for keyword extraction.\n",
    "        nlp (spacy.Language): spaCy language model for linguistic analysis.\n",
    "        min_word_length (int): Minimum length of keywords to consider.\n",
    "        max_keywords (int): Maximum number of keywords to return.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: Refined list of keywords.\n",
    "        List[float]: Corresponding scores for the keywords.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load spaCy model if not provided\n",
    "    if nlp is None:\n",
    "        nlp = spacy.load(\"en_core_web_md\")\n",
    "    \n",
    "    # Initialize KeyBERT\n",
    "    kw_model = KeyBERT()\n",
    "    \n",
    "    # Extract raw keywords\n",
    "    raw_keywords = kw_model.extract_keywords(\n",
    "        text,\n",
    "        keyphrase_ngram_range=(1, 2),  # Allow phrases of 1 to 2 words\n",
    "        top_n=max_keywords * 3,  # Extract more initially for better filtering\n",
    "        use_mmr=True,  # Maximal Marginal Relevance\n",
    "        diversity=0.7  # Increase diversity to balance relevance\n",
    "    )\n",
    "    \n",
    "    # Filter keywords\n",
    "    filtered_keywords = []\n",
    "    valid_pos = {\"NOUN\", \"PROPN\"}  # Focus on nouns and proper nouns for relevance\n",
    "    for keyword, score in raw_keywords:\n",
    "        doc = nlp(keyword)  # Process the keyword with spaCy\n",
    "        \n",
    "        # Check linguistic and quality criteria\n",
    "        if (\n",
    "            len(keyword) >= min_word_length and  # Minimum keyword length\n",
    "            len(keyword.split()) <= 2 and  # Limit to 2-word phrases\n",
    "            all(token.pos_ in valid_pos for token in doc)  # Check POS\n",
    "        ):\n",
    "            filtered_keywords.append((keyword.strip().lower(), score))\n",
    "    \n",
    "    # Remove duplicates and sort by score\n",
    "    unique_keywords = list(dict.fromkeys(filtered_keywords))  # Remove duplicates\n",
    "    unique_keywords.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Limit to max_keywords and store both keywords and scores\n",
    "    keywords = [kw for kw, _ in unique_keywords[:max_keywords]]\n",
    "    scores = [score for _, score in unique_keywords[:max_keywords]]\n",
    "    \n",
    "    return keywords, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: ['backend engineer', 'proficiency backend', 'data mentor', 'job description', 'technology postgresql', 'experience database', 'knowledge cloud', 'mentor junior', 'api design', 'responsibility design', 'kubernetes', 'experience containerization', 'mysql', 'java', 'gcp azure']\n",
      "Scores: [0.651, 0.4771, 0.4636, 0.4554, 0.4319, 0.4262, 0.3726, 0.3447, 0.3348, 0.3321, 0.3166, 0.3149, 0.2747, 0.2714, 0.2557]\n"
     ]
    }
   ],
   "source": [
    "key_words_JobD ,key_words_JobD_scores =extract_relevant_keywords(preprocessed_job_description)\n",
    "print(\"Keywords:\", key_words_JobD)\n",
    "print(\"Scores:\", key_words_JobD_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of keywords in the job description is : 15\n"
     ]
    }
   ],
   "source": [
    "print(f\"the length of keywords in the job description is : {len(key_words_JobD)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: ['backend development', 'asset team', 'development year', 'technology python', 'database', 'skill challenge', 'postgresql', 'machine', 'performance', 'python flask', 'data privacy']\n",
      "Scores: [0.5504, 0.4411, 0.3551, 0.3276, 0.327, 0.3259, 0.3188, 0.2793, 0.2254, 0.2124, 0.1523]\n"
     ]
    }
   ],
   "source": [
    "key_words_interviewee,key_words_interviewee_scores = extract_relevant_keywords(preprocessed_interviewee_responce)\n",
    "\n",
    "print(\"Keywords:\", key_words_interviewee)\n",
    "print(\"Scores:\", key_words_interviewee_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of keywords in the interviewee responce is : 11\n"
     ]
    }
   ],
   "source": [
    "print(f\"the length of keywords in the interviewee responce is : {len(key_words_interviewee)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of total keywords is : 26\n"
     ]
    }
   ],
   "source": [
    "total_keywords=len(key_words_JobD)+len(key_words_interviewee)\n",
    "print(f\"the length of total keywords is : {total_keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get synonyms and calculate similarity score for each synonym     (job description - interviewee)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'backend engineer': {'locomotive_engineer': 0.6, 'technologist': 0.75, 'organise': 0.16666666666666666, 'orchestrate': 0.16666666666666666, 'organize': 0.2, 'direct': 0.11764705882352941, 'applied_scientist': 0.75, 'mastermind': 0.7058823529411765, 'engine_driver': 0.6, 'railroad_engineer': 0.6}, 'proficiency backend': {'technique': 0.26666666666666666}, 'data mentor': {'data_point': 0.36363636363636365, 'datum': 0.36363636363636365, 'information': 0.4, 'wise_man': 1.0}, 'job description': {'chore': 0.75, 'problem': 0.2857142857142857, 'farm_out': 0.18181818181818182, 'subcontract': 0.25, 'line_of_work': 1.0, 'speculate': 0.15384615384615385, 'occupation': 1.0, 'task': 0.8, 'line': 0.3076923076923077, 'Job': 1.0, 'Book_of_Job': 0.26666666666666666, 'business': 0.2857142857142857, 'caper': 0.1111111111111111, 'verbal_description': 1.0}, 'technology postgresql': {'applied_science': 0.35294117647058826, 'engineering_science': 0.35294117647058826, 'engineering': 1.0}, 'experience database': {'have': 0.16666666666666666, 'get': 0.3, 'receive': 0.18181818181818182, 'feel': 0.5, 'go_through': 0.16666666666666666, 'see': 0.125, 'live': 0.18181818181818182, 'know': 0.5333333333333333}, 'knowledge cloud': {'noesis': 1.0, 'cognition': 1.0, 'obnubilate': 0.18181818181818182, 'becloud': 0.18181818181818182, 'corrupt': 0.18181818181818182, 'overcast': 0.7058823529411765, 'mottle': 0.14285714285714285, 'dapple': 0.25, 'taint': 0.14285714285714285, 'swarm': 0.14285714285714285, 'sully': 0.26666666666666666, 'obscure': 0.18181818181818182, 'mist': 0.8235294117647058, 'fog': 0.875, 'befog': 0.18181818181818182, 'haze_over': 0.18181818181818182, 'defile': 0.3076923076923077}, 'mentor junior': {'wise_man': 1.0, 'Jnr': 0.5454545454545454, 'Junior': 1.0, 'next-to-last': 0.16666666666666666, 'third-year': 0.16666666666666666, 'Jr': 0.5454545454545454}, 'api design': {'aim': 0.375, 'intention': 0.375, 'plan': 0.375, 'figure': 0.25, 'designing': 1.0, 'purpose': 0.375, 'blueprint': 0.35294117647058826, 'invention': 0.375, 'innovation': 0.125, 'conception': 0.375, 'intent': 0.375, 'project': 0.7058823529411765, 'excogitation': 0.35294117647058826, 'contrive': 0.14285714285714285, 'pattern': 0.4}, 'responsibility design': {'responsibleness': 0.3076923076923077, 'province': 0.13333333333333333, 'obligation': 1.0, 'duty': 1.0, 'aim': 0.375, 'intention': 0.375, 'plan': 0.375, 'figure': 0.25, 'designing': 1.0, 'purpose': 0.375, 'blueprint': 0.35294117647058826, 'invention': 0.375, 'innovation': 0.125, 'conception': 0.375, 'intent': 0.375, 'project': 0.7058823529411765, 'excogitation': 0.35294117647058826, 'contrive': 0.14285714285714285, 'pattern': 0.4}, 'kubernetes': {}, 'experience containerization': {'have': 0.16666666666666666, 'get': 0.3, 'receive': 0.18181818181818182, 'feel': 0.5, 'go_through': 0.16666666666666666, 'see': 0.125, 'live': 0.18181818181818182, 'know': 0.5333333333333333}, 'mysql': {}, 'java': {'coffee': 0.3076923076923077, 'Java': 1.0}, 'gcp azure': {'bright_blue': 0.16666666666666666, 'lazuline': 1.0, 'sky-blue': 1.0, 'cerulean': 1.0, 'sapphire': 0.2222222222222222}}\n"
     ]
    }
   ],
   "source": [
    "# Function to fetch synonyms for a word using WordNet\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Fetch a set of synonyms for a word using WordNet.\"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "# Function to calculate similarity between words using Wu-Palmer Similarity\n",
    "def get_similarity(word1, word2):\n",
    "    \"\"\"Calculate the similarity between two words using WordNet's Wu-Palmer similarity.\"\"\"\n",
    "    syn1 = wordnet.synsets(word1)\n",
    "    syn2 = wordnet.synsets(word2)\n",
    "    \n",
    "    if syn1 and syn2:\n",
    "        # Calculate similarity between the first synsets of both words\n",
    "        return syn1[0].wup_similarity(syn2[0])  # Wu-Palmer similarity (range: 0 to 1)\n",
    "    return 0  # Return 0 if no similarity found\n",
    "\n",
    "# Function to generate n-grams (1-gram and 2-gram) from the tokens\n",
    "def generate_ngrams(tokens, n=2):\n",
    "    \"\"\"Generate n-grams from the list of tokens.\"\"\"\n",
    "    n_grams = ngrams(tokens, n)\n",
    "    return [' '.join(gram) for gram in n_grams]\n",
    "\n",
    "# Function to combine each bigram with its synonyms and similarity\n",
    "def combine_with_synonyms_and_similarity(doc, n=2):\n",
    "    \"\"\"Combine each bigram in the text with its synonyms and calculate similarity.\"\"\"\n",
    "    combined_dict = {}\n",
    "    tokens = [token.lower() for token in doc]  # Tokenize and lowercase\n",
    "    \n",
    "    n_grams = generate_ngrams(tokens, n)  # Generate n-grams\n",
    "    \n",
    "    for gram in n_grams:\n",
    "        synonyms_with_scores = {}\n",
    "        words_in_bigram = gram.split()  # Split bigram into individual words\n",
    "        \n",
    "        for word in words_in_bigram:\n",
    "            synonyms = get_synonyms(word)  # Get synonyms for the word\n",
    "            \n",
    "            for synonym in synonyms:\n",
    "                if word != synonym:  # Avoid self-similarity\n",
    "                    similarity_score = get_similarity(word, synonym)\n",
    "                    synonyms_with_scores[synonym] = similarity_score\n",
    "        \n",
    "        combined_dict[gram] = synonyms_with_scores  # Store the bigram with synonyms and scores\n",
    "    \n",
    "    return combined_dict\n",
    "\n",
    "\n",
    "# Get the synonyms and similarity for each bigram\n",
    "result = combine_with_synonyms_and_similarity(key_words_JobD, n=1)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git synonyms and calculate similarity score according to threshold  0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function to fetch synonyms for a word using WordNet\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Fetch a set of synonyms for a word using WordNet.\"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "# Function to calculate similarity between words using Wu-Palmer Similarity\n",
    "def get_similarity(word1, word2):\n",
    "    \"\"\"Calculate the similarity between two words using WordNet's Wu-Palmer similarity.\"\"\"\n",
    "    syn1 = wordnet.synsets(word1)\n",
    "    syn2 = wordnet.synsets(word2)\n",
    "    \n",
    "    if syn1 and syn2:\n",
    "        # Calculate similarity between the first synsets of both words\n",
    "        return syn1[0].wup_similarity(syn2[0])  # Wu-Palmer similarity (range: 0 to 1)\n",
    "    return 0  # Return 0 if no similarity found\n",
    "\n",
    "# Function to generate n-grams (1-gram and 2-gram) from the tokens\n",
    "def generate_ngrams(tokens, n=2):\n",
    "    \"\"\"Generate n-grams from the list of tokens.\"\"\"\n",
    "    n_grams = ngrams(tokens, n)\n",
    "    return [' '.join(gram) for gram in n_grams]\n",
    "\n",
    "# Function to combine each bigram with its synonyms and similarity\n",
    "def combine_with_synonyms_and_similarity(key_words, n=2, similarity_threshold=0.98):\n",
    "    \"\"\"Combine each bigram in the text with its synonyms and calculate similarity, filtering by similarity threshold.\"\"\"\n",
    "    combined_dict = {}\n",
    "    \n",
    "    for doc in key_words:\n",
    "        tokens = [token.lower() for token in word_tokenize(doc)]  # Tokenize and lowercase the doc\n",
    "        n_grams = generate_ngrams(tokens, n)  # Generate n-grams\n",
    "        \n",
    "        for gram in n_grams:\n",
    "            synonyms_with_scores = {}\n",
    "            words_in_bigram = gram.split()  # Split bigram into individual words\n",
    "            \n",
    "            for word in words_in_bigram:\n",
    "                synonyms = get_synonyms(word)  # Get synonyms for the word\n",
    "                \n",
    "                for synonym in synonyms:\n",
    "                    if word != synonym:  # Avoid self-similarity\n",
    "                        similarity_score = get_similarity(word, synonym)\n",
    "                        # Only include synonyms with similarity >= 0.98\n",
    "                        if similarity_score >= similarity_threshold:\n",
    "                            synonyms_with_scores[synonym] = similarity_score\n",
    "            \n",
    "            if synonyms_with_scores:  # Only add to dictionary if there are valid synonyms\n",
    "                combined_dict[gram] = synonyms_with_scores  # Store the bigram with synonyms and scores\n",
    "    \n",
    "    return combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data mentor': {'wise_man': 1.0},\n",
       " 'job description': {'line_of_work': 1.0,\n",
       "  'occupation': 1.0,\n",
       "  'Job': 1.0,\n",
       "  'verbal_description': 1.0},\n",
       " 'technology postgresql': {'engineering': 1.0},\n",
       " 'knowledge cloud': {'noesis': 1.0, 'cognition': 1.0},\n",
       " 'mentor junior': {'wise_man': 1.0, 'Junior': 1.0},\n",
       " 'api design': {'designing': 1.0},\n",
       " 'responsibility design': {'obligation': 1.0, 'duty': 1.0, 'designing': 1.0},\n",
       " 'gcp azure': {'lazuline': 1.0, 'sky-blue': 1.0, 'cerulean': 1.0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Synonyms_similarity_JobD = combine_with_synonyms_and_similarity(key_words_JobD, n=2, similarity_threshold=0.90)\n",
    "Synonyms_similarity_JobD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'asset team': {'plus': 1.0},\n",
       " 'development year': {'twelvemonth': 1.0, 'yr': 1.0},\n",
       " 'technology python': {'engineering': 1.0, 'Python': 1.0},\n",
       " 'skill challenge': {'acquirement': 1.0},\n",
       " 'python flask': {'Python': 1.0},\n",
       " 'data privacy': {'seclusion': 1.0}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Synonyms_similarity_interviewee = combine_with_synonyms_and_similarity(key_words_interviewee, n=2, similarity_threshold=0.90)\n",
    "Synonyms_similarity_interviewee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of total keywords with similarity_thresholding in job description and interviewee responsce is : 14\n"
     ]
    }
   ],
   "source": [
    "total_Synonyms_similarity_with_threshold = len(Synonyms_similarity_JobD)+len(Synonyms_similarity_interviewee)\n",
    "print(f\"the length of total keywords with similarity_thresholding in job description and interviewee responsce is : {total_Synonyms_similarity_with_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of total keywords in job description and interviewee responsce is : 26\n"
     ]
    }
   ],
   "source": [
    "print(f\"the length of total keywords in job description and interviewee responsce is : {total_keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similarity percentage between job description and interviewee responce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Percentage using Hash Map: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate percentage similarity using hash maps\n",
    "def calculate_similarity_percentage_using_hash_map(dict1, dict2):\n",
    "    count = 0\n",
    "\n",
    "    # Create a hash map for synonyms from both dictionaries\n",
    "    synonym_map_jobd = {}\n",
    "    synonym_map_interviewee = {}\n",
    "\n",
    "    # Fill the synonym map for Synonyms_similarity_JobD\n",
    "    for key, synonyms in dict1.items():\n",
    "        for synonym in synonyms:\n",
    "            if synonym not in synonym_map_jobd:\n",
    "                synonym_map_jobd[synonym] = set()\n",
    "            synonym_map_jobd[synonym].add(key)\n",
    "\n",
    "    # Fill the synonym map for Synonyms_similarity_interviewee\n",
    "    for key, synonyms in dict2.items():\n",
    "        for synonym in synonyms:\n",
    "            if synonym not in synonym_map_interviewee:\n",
    "                synonym_map_interviewee[synonym] = set()\n",
    "            synonym_map_interviewee[synonym].add(key)\n",
    "\n",
    "    # Loop through the synonyms in Synonyms_similarity_JobD\n",
    "    for synonym, keywords in synonym_map_jobd.items():\n",
    "        if synonym in synonym_map_interviewee:\n",
    "            matching_keywords_jobd = keywords\n",
    "            matching_keywords_interviewee = synonym_map_interviewee[synonym]\n",
    "            # Increment count for each matching synonym\n",
    "            count += len(matching_keywords_jobd & matching_keywords_interviewee)  # Intersection of matching keywords\n",
    "\n",
    "            # Remove the synonym from interviewee map to avoid double counting\n",
    "            del synonym_map_interviewee[synonym]\n",
    "\n",
    "    # Calculate the percentage similarity\n",
    "    similarity_percentage = (count / total_keywords) * 100\n",
    "    return similarity_percentage\n",
    "\n",
    "# Calculate similarity percentage using hash maps\n",
    "similarity_percentage = calculate_similarity_percentage_using_hash_map(Synonyms_similarity_JobD, Synonyms_similarity_interviewee)\n",
    "print(f\"Similarity Percentage using Hash Map: {similarity_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
