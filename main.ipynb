{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "\n",
    "## Load video and audio recording libraries\n",
    "import cv2\n",
    "import pyaudio\n",
    "import wave\n",
    "import threading\n",
    "import subprocess\n",
    "\n",
    "## Load speech recognition tools\n",
    "import whisper\n",
    "\n",
    "## Define the class for handling video recording\n",
    "from typing import Optional, Dict, Union\n",
    "from pathlib import Path\n",
    "\n",
    "## Load Natural Language Processing (NLP) tools\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.util import bigrams\n",
    "\n",
    "## Setup logging\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intelligent Interview Recording System\n",
    "#### This module provides a robust screen recording solution with audio-video capture \n",
    "#### and merging capabilities, designed for interview analysis and recording.\n",
    "\n",
    "### Key Features:\n",
    "- Simultaneous video and audio recording\n",
    "- Unique filename generation with timestamps\n",
    "- Automatic file merging using FFmpeg\n",
    "- Configurable output directory\n",
    "- Error handling for camera and audio device access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class for recording functions to create a simplified version that starts and stops recording when specific buttons are pressed####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScreenRecorder:\n",
    "    \"\"\"\n",
    "    A comprehensive screen recording class that captures video and audio \n",
    "    simultaneously and merges them into a single output file.\n",
    "\n",
    "    Attributes:\n",
    "        output_dir (str): Directory to save recorded files\n",
    "        FORMAT (int): Audio format (16-bit integer)\n",
    "        CHANNELS (int): Number of audio channels (mono)\n",
    "        RATE (int): Audio sample rate\n",
    "        CHUNK (int): Audio buffer size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dir='recordings'):\n",
    "        \"\"\"\n",
    "        Initialize the ScreenRecorder with configuration settings.\n",
    "\n",
    "        Args:\n",
    "            output_dir (str, optional): Directory to save recordings. \n",
    "                                        Defaults to 'recordings'.\n",
    "        \"\"\"\n",
    "        # Ensure output directory exists\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Audio recording configuration\n",
    "        self.FORMAT = pyaudio.paInt16  # 16-bit audio\n",
    "        self.CHANNELS = 1  # Mono audio\n",
    "        self.RATE = 44100  # Standard audio sample rate\n",
    "        self.CHUNK = 1024  # Audio buffer size\n",
    "\n",
    "        # File path placeholders\n",
    "        self.video_path = None      # Raw video file path\n",
    "        self.audio_path = None      # Raw audio file path\n",
    "        self.final_output_path = None  # Merged video file path\n",
    "\n",
    "        # Recording state management\n",
    "        self.recording = False\n",
    "        self.capture = None\n",
    "        self.video_writer = None\n",
    "        self.audio_stream = None\n",
    "        self.audio_frames = []\n",
    "\n",
    "    def start_recording(self):\n",
    "        \"\"\"\n",
    "        Initiate screen recording process.\n",
    "\n",
    "        Responsibilities:\n",
    "        - Check if recording is already in progress\n",
    "        - Generate unique filenames\n",
    "        - Initialize video capture\n",
    "        - Initialize audio capture\n",
    "        - Start recording threads\n",
    "\n",
    "        Raises:\n",
    "            SystemError: If camera cannot be accessed\n",
    "        \"\"\"\n",
    "        # Prevent multiple recording sessions\n",
    "        if self.recording:\n",
    "            print(\"Recording is already in progress.\")\n",
    "            return\n",
    "        \n",
    "        # Generate unique timestamps for filenames\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.video_path = os.path.join(self.output_dir, f\"video_{timestamp}.avi\")\n",
    "        self.audio_path = os.path.join(self.output_dir, f\"audio_{timestamp}.wav\")\n",
    "        self.final_output_path = os.path.join(self.output_dir, f\"final_output_{timestamp}.mp4\")\n",
    "\n",
    "        # Initialize video capture\n",
    "        self.capture = cv2.VideoCapture(0)\n",
    "        if not self.capture.isOpened():\n",
    "            print(\"Error: Unable to access the camera.\")\n",
    "            return\n",
    "\n",
    "        # Retrieve video parameters\n",
    "        frame_width = int(self.capture.get(3))   # Camera width\n",
    "        frame_height = int(self.capture.get(4))  # Camera height\n",
    "        frame_rate = int(self.capture.get(5)) or 30  # Frame rate or default\n",
    "\n",
    "        # Create video writer object\n",
    "        self.video_writer = cv2.VideoWriter(\n",
    "            self.video_path,\n",
    "            cv2.VideoWriter_fourcc(*\"XVID\"),  # Video codec\n",
    "            frame_rate,\n",
    "            (frame_width, frame_height)\n",
    "        )\n",
    "\n",
    "        # Initialize audio recording\n",
    "        self.audio_frames = []\n",
    "        self.audio_stream = pyaudio.PyAudio().open(\n",
    "            format=self.FORMAT,\n",
    "            channels=self.CHANNELS,\n",
    "            rate=self.RATE,\n",
    "            input=True,\n",
    "            frames_per_buffer=self.CHUNK\n",
    "        )\n",
    "\n",
    "        # Start recording threads\n",
    "        self.recording = True\n",
    "        threading.Thread(target=self._record_video, daemon=True).start()\n",
    "        threading.Thread(target=self._record_audio, daemon=True).start()\n",
    "\n",
    "        print(f\"Recording started. Video will be saved to: {self.video_path}\")\n",
    "        print(f\"Audio will be saved to: {self.audio_path}\")\n",
    "\n",
    "    def _record_video(self):\n",
    "        \"\"\"\n",
    "        Internal method to continuously capture video frames.\n",
    "        Runs in a separate thread during recording.\n",
    "        \"\"\"\n",
    "        while self.recording:\n",
    "            ret, frame = self.capture.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Unable to read from camera.\")\n",
    "                break\n",
    "            self.video_writer.write(frame)\n",
    "\n",
    "    def _record_audio(self):\n",
    "        \"\"\"\n",
    "        Internal method to continuously capture audio frames.\n",
    "        Runs in a separate thread during recording.\n",
    "        \"\"\"\n",
    "        while self.recording:\n",
    "            data = self.audio_stream.read(self.CHUNK)\n",
    "            self.audio_frames.append(data)\n",
    "\n",
    "    def stop_recording(self):\n",
    "        \"\"\"\n",
    "        Stop the recording process and merge audio-video files.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Paths for video, audio, and final merged file\n",
    "        \"\"\"\n",
    "        # Prevent stopping non-existent recording\n",
    "        if not self.recording:\n",
    "            print(\"No recording in progress.\")\n",
    "            return None, None, None\n",
    "\n",
    "        # Stop recording\n",
    "        self.recording = False\n",
    "\n",
    "        # Release system resources\n",
    "        self.capture.release()\n",
    "        self.video_writer.release()\n",
    "\n",
    "        # Save audio file\n",
    "        with wave.open(self.audio_path, 'wb') as wf:\n",
    "            wf.setnchannels(self.CHANNELS)\n",
    "            wf.setsampwidth(pyaudio.PyAudio().get_sample_size(self.FORMAT))\n",
    "            wf.setframerate(self.RATE)\n",
    "            wf.writeframes(b''.join(self.audio_frames))\n",
    "\n",
    "        # Merge video and audio\n",
    "        self._merge_video_audio()\n",
    "\n",
    "        # Output recording details\n",
    "        print(f\"Recording stopped.\")\n",
    "        print(f\"Video path: {self.video_path}\")\n",
    "        print(f\"Audio path: {self.audio_path}\")\n",
    "        print(f\"Final output path: {self.final_output_path}\")\n",
    "\n",
    "        return self.video_path, self.audio_path, self.final_output_path\n",
    "\n",
    "    def _merge_video_audio(self):\n",
    "        \"\"\"\n",
    "        Merge video and audio files using FFmpeg.\n",
    "\n",
    "        Handles:\n",
    "        - Video and audio merging\n",
    "        - Error checking for FFmpeg availability\n",
    "        - Codec conversion\n",
    "\n",
    "        Raises:\n",
    "            subprocess.CalledProcessError: FFmpeg command execution error\n",
    "            FileNotFoundError: FFmpeg not installed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # FFmpeg command for merging files\n",
    "            merge_command = [\n",
    "                'ffmpeg',\n",
    "                '-i', self.video_path,      # Input video\n",
    "                '-i', self.audio_path,      # Input audio\n",
    "                '-c:v', 'copy',             # Copy video codec\n",
    "                '-c:a', 'aac',              # Convert audio to AAC\n",
    "                '-shortest',                # Match shorter file duration\n",
    "                self.final_output_path      # Output merged file\n",
    "            ]\n",
    "            \n",
    "            # Execute FFmpeg merge\n",
    "            subprocess.run(merge_command, check=True)\n",
    "            print(\"Successfully merged video and audio\")\n",
    "        \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error merging video and audio: {e}\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"FFmpeg not found. Please install FFmpeg to merge video and audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started. Video will be saved to: recordings\\video_20241203_234045.avi\n",
      "Audio will be saved to: recordings\\audio_20241203_234045.wav\n",
      "Successfully merged video and audio\n",
      "Recording stopped.\n",
      "Video path: recordings\\video_20241203_234045.avi\n",
      "Audio path: recordings\\audio_20241203_234045.wav\n",
      "Final output path: recordings\\final_output_20241203_234045.mp4\n"
     ]
    }
   ],
   "source": [
    "recorder = ScreenRecorder()\n",
    "input(\"Press Enter to start recording...\")\n",
    "recorder.start_recording()\n",
    "    \n",
    "input(\"Press Enter to stop recording...\")\n",
    "video_path, audio_path, final_output_path = recorder.stop_recording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recordings\\video_20241203_234045.avi    recordings\\audio_20241203_234045.wav       recordings\\final_output_20241203_234045.mp4\n"
     ]
    }
   ],
   "source": [
    "print(video_path,\"  \", audio_path, \"     \",final_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply OpenAI's Whisper model for handling audio/video transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperTranscriber:\n",
    "    \"\"\"\n",
    "    A class to handle audio/video transcription using OpenAI's Whisper model.\n",
    "    Supports multiple input formats and handles conversion to WAV using FFmpeg.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the transcriber with the 'base' Whisper model.\n",
    "        \"\"\"\n",
    "        self.setup_logging()\n",
    "        self.model = whisper.load_model(\"base\")\n",
    "        self.language = \"en\"\n",
    "\n",
    "    def setup_logging(self) -> None:\n",
    "        \"\"\"Configure logging for the transcriber.\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def convert_to_wav(self, input_file: Union[str, Path], output_dir: Optional[str] = None) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Convert input audio/video file to WAV format using FFmpeg.\n",
    "        \n",
    "        Args:\n",
    "            input_file (Union[str, Path]): Path to input audio/video file\n",
    "            output_dir (Optional[str]): Directory for output WAV file\n",
    "            \n",
    "        Returns:\n",
    "            Optional[str]: Path to output WAV file if successful, None otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input_path = Path(input_file)\n",
    "            if not input_path.exists():\n",
    "                self.logger.error(f\"Input file not found: {input_file}\")\n",
    "                return None\n",
    "            \n",
    "            # Determine output path\n",
    "            if output_dir:\n",
    "                output_path = Path(output_dir) / f\"{input_path.stem}.wav\"\n",
    "            else:\n",
    "                output_path = input_path.with_suffix('.wav')\n",
    "                \n",
    "            # Create output directory if it doesn't exist\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # FFmpeg command for conversion\n",
    "            command = [\n",
    "                'ffmpeg',\n",
    "                '-i', str(input_path),\n",
    "                '-ar', '16000',  # Sample rate 16kHz\n",
    "                '-ac', '1',      # Mono audio\n",
    "                '-c:a', 'pcm_s16le',  # 16-bit PCM encoding\n",
    "                str(output_path),\n",
    "                '-y'  # Overwrite output file if exists\n",
    "            ]\n",
    "            \n",
    "            self.logger.info(f\"Converting {input_path} to WAV format\")\n",
    "            result = subprocess.run(\n",
    "                command,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.PIPE,\n",
    "                text=True\n",
    "            )\n",
    "            \n",
    "            if result.returncode != 0:\n",
    "                self.logger.error(f\"FFmpeg conversion failed: {result.stderr}\")\n",
    "                return None\n",
    "                \n",
    "            self.logger.info(\"Conversion successful\")\n",
    "            return str(output_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during conversion: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def transcribe(self, \n",
    "                  input_file: Union[str, Path], \n",
    "                  output_dir: Optional[str] = None,\n",
    "                  cleanup: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Transcribe audio/video file using Whisper.\n",
    "        \n",
    "        Args:\n",
    "            input_file (Union[str, Path]): Path to input audio/video file\n",
    "            output_dir (Optional[str]): Directory for temporary WAV file\n",
    "            cleanup (bool): Whether to delete temporary WAV file after transcription\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Transcription result containing text and other metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert to WAV if input is not already WAV\n",
    "            input_path = Path(input_file)\n",
    "            if input_path.suffix.lower() != '.wav':\n",
    "                self.logger.info(\"Converting input file to WAV format\")\n",
    "                wav_file = self.convert_to_wav(input_file, output_dir)\n",
    "                if not wav_file:\n",
    "                    raise RuntimeError(\"Failed to convert input file to WAV format\")\n",
    "            else:\n",
    "                wav_file = str(input_path)\n",
    "            \n",
    "            # Perform transcription\n",
    "            self.logger.info(\"Starting transcription\")\n",
    "            result = self.model.transcribe(wav_file, language=self.language)\n",
    "            self.logger.info(\"Transcription completed successfully\")\n",
    "            \n",
    "            # Cleanup temporary WAV file if requested\n",
    "            if cleanup and input_path.suffix.lower() != '.wav':\n",
    "                try:\n",
    "                    os.remove(wav_file)\n",
    "                    self.logger.info(f\"Cleaned up temporary WAV file: {wav_file}\")\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Failed to cleanup temporary file: {str(e)}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Transcription failed: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohamed Walid\\AppData\\Roaming\\Python\\Python311\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "2024-12-04 00:08:05,035 - INFO - Converting input file to WAV format\n",
      "2024-12-04 00:08:05,036 - INFO - Converting G:\\Intelligent AI-based Interview Analysis\\recordings\\final_output_20241203_234045.mp4 to WAV format\n",
      "2024-12-04 00:08:05,550 - INFO - Conversion successful\n",
      "2024-12-04 00:08:05,550 - INFO - Starting transcription\n",
      "C:\\Users\\Mohamed Walid\\AppData\\Roaming\\Python\\Python311\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "2024-12-04 00:08:07,654 - INFO - Transcription completed successfully\n",
      "2024-12-04 00:08:07,655 - INFO - Cleaned up temporary WAV file: G:\\Intelligent AI-based Interview Analysis\\recordings\\final_output_20241203_234045.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Welcome today we are talking about our graduation project. In my final year I do edu's my teammates in the university. It is my best interview analysis for Candidate Recognition.\n"
     ]
    }
   ],
   "source": [
    "# Initialize transcriber\n",
    "transcriber = WhisperTranscriber()\n",
    "    \n",
    "input_file = r\"G:\\Intelligent AI-based Interview Analysis\\recordings\\final_output_20241203_234045.mp4\"\n",
    "try:\n",
    "    result = transcriber.transcribe(input_file)\n",
    "        \n",
    "    # Store the transcribed text in a variable\n",
    "    transcribed_text = result['text']\n",
    "        \n",
    "    # Print the transcribed text\n",
    "    print(f\"Transcription: {transcribed_text}\")\n",
    "        \n",
    "    # You can now use transcribed_text for further processing\n",
    "    # For example, save to a file or use in other functions\n",
    "except Exception as e:\n",
    "    print(f\"Transcription failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing transcribed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Mohamed\n",
      "[nltk_data]     Walid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mohamed\n",
      "[nltk_data]     Walid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Mohamed\n",
      "[nltk_data]     Walid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! \" # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ ` { | } ~ "
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "for char in string.punctuation:\n",
    "    print(char,end= \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_table = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `lemmatization` here in preprocessing as it reduces words to their base , considering the context and the actual meaning. which will be useful for `keyword extraction` as it help maintain the correct semantic meaning, improving the accuracy .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_wordnet_pos() =>\n",
    "#### function used when preprocessing text for tasks like lemmatization, where words need to be reduced to their base forms based on their correct part of speech\n",
    "#### takes a part-of-speech (POS) tag from the Penn Treebank tag set and maps it to a corresponding POS tag used by the WordNet lexical database,  The function ensures that the POS tag is translated into a format compatible with WordNet's system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV  # Adverb\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Text: welcome today talk graduation project final year edus teammate university best interview analysis candidate recognition\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r'\\d+', '', text)       # Remove numbers\n",
    "    text = text.translate(translated_table)\n",
    "\n",
    "    text_tokens = word_tokenize(text)\n",
    "\n",
    "    filtered_words=[word for word in text_tokens if word not in stop_words ]\n",
    "    # lemmatization => transforming words to their base or dictionary form\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "    lemma_words = []\n",
    "    for word in filtered_words:\n",
    "        pos_tag = nltk.pos_tag([word])[0][1]  # Get POS tag for each word\n",
    "        wordnet_pos = get_wordnet_pos(pos_tag)  # Map POS to WordNet POS\n",
    "        lemma_word = lemmatizer.lemmatize(word, pos=wordnet_pos)  # Lemmatize using WordNet POS\n",
    "        lemma_words.append(lemma_word)\n",
    "\n",
    "    processed_text = ' '.join(lemma_words)\n",
    "    return processed_text\n",
    "\n",
    "preprocessed_text = preprocess_text(transcribed_text)\n",
    "print(f\"Preprocessed Text: {preprocessed_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract important keyboard from Preprocessed Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2024-12-04 00:10:20,106 - INFO - Use pytorch device_name: cpu\n",
      "2024-12-04 00:10:20,108 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant job keywords: ['interview analysis', 'analysis candidate', 'candidate recognition', 'interview', 'edus teammate', 'graduation project', 'candidate']\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Load spaCy model for POS tagging\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample preprocessed text, replace this with your actual preprocessed text\n",
    "text = preprocessed_text\n",
    "# Initialize KeyBERT model\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "# Extract keywords with KeyBERT\n",
    "raw_keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=10)\n",
    "raw_keywords = [kw[0] for kw in raw_keywords]  # Keep only the keywords without scores\n",
    "\n",
    "# Filter keywords using spaCy for relevant parts of speech\n",
    "filtered_keywords = []\n",
    "for keyword in raw_keywords:\n",
    "    doc = nlp(keyword)\n",
    "    # Check if all tokens in the keyword are either NOUN or PROPN\n",
    "    if all(token.pos_ in {\"NOUN\", \"PROPN\"} for token in doc):\n",
    "        filtered_keywords.append(keyword)\n",
    "\n",
    "print(\"Relevant job keywords:\", filtered_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['interview analysis',\n",
       " 'analysis candidate',\n",
       " 'candidate recognition',\n",
       " 'interview',\n",
       " 'edus teammate',\n",
       " 'graduation project',\n",
       " 'candidate']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "# Function to fetch synonyms for a word using WordNet\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Fetch a set of synonyms for a word using WordNet.\"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "# Function to generate n-grams (1-gram and 2-gram) from the tokens\n",
    "def generate_ngrams(tokens, n=2):\n",
    "    \"\"\"Generate n-grams from the list of tokens.\"\"\"\n",
    "    n_grams = ngrams(tokens, n)\n",
    "    return [' '.join(gram) for gram in n_grams]\n",
    "\n",
    "# Function to combine each word or phrase (bigram) with its synonyms\n",
    "def combine_with_synonyms(doc, n=2):\n",
    "    \"\"\"Combine each word in the text with its synonyms (including bigrams).\"\"\"\n",
    "    combined_dict = {}\n",
    "    tokens = [token.text.lower() for token in doc]  # Get list of tokens from doc\n",
    "    n_grams = generate_ngrams(tokens, n)  # Generate n-grams (1 or 2)\n",
    "    \n",
    "    for gram in n_grams:\n",
    "        synonyms_for_bigram = set()\n",
    "        words_in_bigram = gram.split()  # Split bigram into individual words\n",
    "        \n",
    "        # Get synonyms for each word in the bigram\n",
    "        for word in words_in_bigram:\n",
    "            synonyms_for_bigram.update(get_synonyms(word))\n",
    "        \n",
    "        combined_dict[gram] = list(synonyms_for_bigram)  # Store the synonyms for the bigram\n",
    "    \n",
    "    return combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Get the synonyms for each token or bigram in the processed text\n",
    "result = combine_with_synonyms(doc, n=2)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the synonyms for each word in the text and the similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function to fetch synonyms for a word using WordNet\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Fetch a set of synonyms for a word using WordNet.\"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "# Function to calculate similarity between words using Wu-Palmer Similarity\n",
    "def get_similarity(word1, word2):\n",
    "    \"\"\"Calculate the similarity between two words using WordNet's Wu-Palmer similarity.\"\"\"\n",
    "    syn1 = wordnet.synsets(word1)\n",
    "    syn2 = wordnet.synsets(word2)\n",
    "    \n",
    "    if syn1 and syn2:\n",
    "        # Calculate similarity between the first synsets of both words\n",
    "        return syn1[0].wup_similarity(syn2[0])  # Wu-Palmer similarity (range: 0 to 1)\n",
    "    return 0  # Return 0 if no similarity found\n",
    "\n",
    "# Function to generate n-grams (1-gram and 2-gram) from the tokens\n",
    "def generate_ngrams(tokens, n=2):\n",
    "    \"\"\"Generate n-grams from the list of tokens.\"\"\"\n",
    "    n_grams = ngrams(tokens, n)\n",
    "    return [' '.join(gram) for gram in n_grams]\n",
    "\n",
    "# Function to combine each bigram with its synonyms and similarity\n",
    "def combine_with_synonyms_and_similarity(doc, n=2, similarity_threshold=0.98):\n",
    "    \"\"\"Combine each bigram in the text with its synonyms and calculate similarity, filtering by similarity threshold.\"\"\"\n",
    "    combined_dict = {}\n",
    "    tokens = [token.lower() for token in word_tokenize(doc)]  # Tokenize and lowercase the doc\n",
    "    n_grams = generate_ngrams(tokens, n)  # Generate n-grams\n",
    "    \n",
    "    for gram in n_grams:\n",
    "        synonyms_with_scores = {}\n",
    "        words_in_bigram = gram.split()  # Split bigram into individual words\n",
    "        \n",
    "        for word in words_in_bigram:\n",
    "            synonyms = get_synonyms(word)  # Get synonyms for the word\n",
    "            \n",
    "            for synonym in synonyms:\n",
    "                if word != synonym:  # Avoid self-similarity\n",
    "                    similarity_score = get_similarity(word, synonym)\n",
    "                    # Only include synonyms with similarity >= 0.98\n",
    "                    if similarity_score >= similarity_threshold:\n",
    "                        synonyms_with_scores[synonym] = similarity_score\n",
    "        \n",
    "        if synonyms_with_scores:  # Only add to dictionary if there are valid synonyms\n",
    "            combined_dict[gram] = synonyms_with_scores  # Store the bigram with synonyms and scores\n",
    "    \n",
    "    return combined_dict\n",
    "\n",
    "# Example Usage\n",
    "doc = \"The quick brown fox jumps over the lazy dog\"\n",
    "result = combine_with_synonyms_and_similarity(doc, n=2, similarity_threshold=0.98)\n",
    "print(result)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
