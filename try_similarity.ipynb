{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description='''\n",
    "Job Description: Senior Backend Engineer\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "Design, develop, and maintain robust and scalable backend systems.\n",
    "Collaborate with frontend and mobile teams to build seamless user experiences.\n",
    "Optimize database performance and write efficient SQL queries.\n",
    "Implement robust security measures to protect sensitive data.\n",
    "Mentor junior engineers and foster a culture of continuous learning.\n",
    "Required Skills:\n",
    "\n",
    "Strong proficiency in backend programming languages (e.g., Python, Node.js, Ruby on Rails, Java).\n",
    "Experience with database technologies (e.g., PostgreSQL, MySQL, MongoDB).\n",
    "Solid understanding of RESTful API design and development.\n",
    "Knowledge of cloud platforms (e.g., AWS, GCP, Azure).\n",
    "Experience with containerization technologies (e.g., Docker, Kubernetes).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "interviewee_responce='''\n",
    "I've been passionate about backend development for 3 years, and I'm excited to apply my skills to challenging projects.\n",
    "At my previous role at egy_tech, I was responsible for building a scalable API that handled 100 requests per second.\n",
    "I utilized [Specific technologies, e.g., Python, Flask, PostgreSQL] to optimize performance and ensure reliability.\n",
    "\n",
    "I'm particularly interested in your company's focus on [database, data privacy, machine learning, Azuru].\n",
    "I've been exploring Node.js and believe it could be a valuable asset to your team.\n",
    "I'm eager to contribute to innovative projects and learn from experienced engineers.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocesssing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Mohamed\n",
      "[nltk_data]     Walid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mohamed\n",
      "[nltk_data]     Walid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Mohamed\n",
      "[nltk_data]     Walid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Mohamed\n",
      "[nltk_data]     Walid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Mohamed Walid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "translated_table = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV  # Adverb\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'\\d+', '', text)       # Remove numbers\n",
    "    text = text.translate(translated_table)\n",
    "\n",
    "    text_tokens = word_tokenize(text)\n",
    "\n",
    "    filtered_words=[word for word in text_tokens if word not in stop_words ]\n",
    "    # lemmatization => transforming words to their base or dictionary form\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "    lemma_words = []\n",
    "    for word in filtered_words:\n",
    "        pos_tag = nltk.pos_tag([word])[0][1]  # Get POS tag for each word\n",
    "        wordnet_pos = get_wordnet_pos(pos_tag)  # Map POS to WordNet POS\n",
    "        lemma_word = lemmatizer.lemmatize(word, pos=wordnet_pos)  # Lemmatize using WordNet POS\n",
    "        lemma_words.append(lemma_word)\n",
    "\n",
    "    processed_text = ' '.join(lemma_words)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed job description : job description senior backend engineer responsibility design develop maintain robust scalable backend system collaborate frontend mobile team build seamless user experience optimize database performance write efficient sql query implement robust security measure protect sensitive data mentor junior engineer foster culture continuous learn require skill strong proficiency backend program language eg python nodejs ruby rail java experience database technology eg postgresql mysql mongodb solid understand restful api design development knowledge cloud platform eg aws gcp azure experience containerization technology eg docker kubernetes\n"
     ]
    }
   ],
   "source": [
    "preprocessed_job_description = preprocess_text(job_description)\n",
    "print(f\"Preprocessed job description : {preprocessed_job_description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed interviewee responce : ive passionate backend development year im excite apply skill challenge project previous role egytech responsible building scalable api handle request per second utilized specific technology eg python flask postgresql optimize performance ensure reliability im particularly interested company focus database data privacy machine learn azuru ive explore nodejs believe could valuable asset team im eager contribute innovative project learn experienced engineer\n"
     ]
    }
   ],
   "source": [
    "preprocessed_interviewee_responce= preprocess_text(interviewee_responce)\n",
    "print(f\"Preprocessed interviewee responce : {preprocessed_interviewee_responce}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extract important keywords from job description and interviewee responce**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_keywords(text, nlp=None):\n",
    "    \"\"\"\n",
    "    Extract relevant keywords with robust filtering and customization options.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text for keyword extraction.\n",
    "        nlp (spacy.Language): spaCy language model for linguistic analysis.\n",
    "        top_n (int): Maximum number of keywords to return.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Refined list of keywords.\n",
    "        List[float]: Corresponding scores for the keywords.\n",
    "    \"\"\"\n",
    "    # Load spaCy model if not provided\n",
    "    if nlp is None:\n",
    "        nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "    # Initialize KeyBERT\n",
    "    kw_model = KeyBERT()\n",
    "\n",
    "    # Extract keywords with KeyBERT\n",
    "    raw_keywords = kw_model.extract_keywords(\n",
    "        text, keyphrase_ngram_range=(1, 3), stop_words=\"english\", top_n=25\n",
    "    )\n",
    "\n",
    "    # Filter keywords\n",
    "    filtered_keywords = []\n",
    "    filtered_scores = []\n",
    "    valid_pos = {\"NOUN\", \"PROPN\",\"VERB\"}  # Focus on nouns and proper nouns for relevance and verbs\n",
    "\n",
    "    for keyword, score in raw_keywords:\n",
    "        doc = nlp(keyword)  # Process the keyword with spaCy\n",
    "\n",
    "        # Check if all tokens in the keyword are either NOUN or PROPN\n",
    "        if all(token.pos_ in valid_pos for token in doc):\n",
    "            filtered_keywords.append(keyword)\n",
    "            filtered_scores.append(score)\n",
    "\n",
    "    return filtered_keywords, filtered_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: ['backend engineer', 'backend engineer responsibility', 'backend', 'proficiency backend', 'experience database technology', 'data mentor junior', 'data mentor', 'proficiency backend program', 'job description', 'develop maintain', 'development knowledge cloud', 'knowledge cloud platform', 'backend program', 'technology postgresql', 'backend program language']\n",
      "Scores: [0.651, 0.5717, 0.4895, 0.4771, 0.4766, 0.4688, 0.4636, 0.458, 0.4554, 0.4471, 0.4435, 0.4418, 0.4369, 0.4319, 0.4316]\n",
      "The length of keywords in the job description is: 15\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "key_words_JobD, key_words_JobD_scores = extract_relevant_keywords(preprocessed_job_description)\n",
    "print(\"Keywords:\", key_words_JobD)\n",
    "print(\"Scores:\", key_words_JobD_scores)\n",
    "print(f\"The length of keywords in the job description is: {len(key_words_JobD)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: ['passionate backend development', 'backend development', 'backend development year', 'project learn experienced', 'experienced engineer', 'backend', 'asset team', 'role egytech', 'learn experienced engineer', 'development', 'engineer', 'project learn', 'nodejs', 'learn experienced']\n",
      "Scores: [0.6362, 0.5504, 0.5419, 0.5132, 0.4885, 0.4837, 0.4411, 0.4311, 0.4145, 0.4111, 0.409, 0.4047, 0.3986, 0.3943]\n",
      "the length of keywords in the interviewee responce is : 14\n"
     ]
    }
   ],
   "source": [
    "key_words_interviewee,key_words_interviewee_scores = extract_relevant_keywords(preprocessed_interviewee_responce)\n",
    "\n",
    "print(\"Keywords:\", key_words_interviewee)\n",
    "print(\"Scores:\", key_words_interviewee_scores)\n",
    "print(f\"the length of keywords in the interviewee responce is : {len(key_words_interviewee)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Synonyms of each word in the keyword**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function to fetch synonyms for a word using WordNet\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Fetch a set of synonyms for a word using WordNet.\"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def get_similarity(word1, word2):\n",
    "    \"\"\"Calculate the similarity between two words using WordNet's Wu-Palmer similarity.\"\"\"\n",
    "    syn1 = wordnet.synsets(word1)\n",
    "    syn2 = wordnet.synsets(word2)\n",
    "    \n",
    "    if syn1 and syn2:\n",
    "        # Calculate similarity between the first synsets of both words\n",
    "        return syn1[0].wup_similarity(syn2[0])  # Wu-Palmer similarity (range: 0 to 1)\n",
    "    return 0  # Return 0 if no similarity found\n",
    "\n",
    "# Function to generate n-grams (1-gram and 2-gram) from the tokens\n",
    "def generate_ngrams(tokens, n=2):\n",
    "    \"\"\"Generate n-grams from the list of tokens.\"\"\"\n",
    "    n_grams = ngrams(tokens, n)\n",
    "    return [' '.join(gram) for gram in n_grams]\n",
    "\n",
    "# Function to combine each bigram with its synonyms and calculate similarity with a threshold\n",
    "def combine_with_synonyms_and_similarity(doc, n=1, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Combine each bigram in the text with its synonyms and calculate similarity.\n",
    "    Only include synonyms with a similarity score above the threshold.\n",
    "    \"\"\"\n",
    "    combined_dict = {}\n",
    "    tokens = [token.lower() for token in doc]  # Lowercase tokens (assuming `doc` is a list of tokens)\n",
    "    n_grams = generate_ngrams(tokens, n)  # Generate n-grams\n",
    "    \n",
    "    for gram in n_grams:\n",
    "        synonyms_with_scores = {}\n",
    "        words_in_bigram = gram.split()  # Split bigram into individual words\n",
    "        \n",
    "        for word in words_in_bigram:\n",
    "            synonyms = get_synonyms(word)  # Get synonyms for the word\n",
    "            \n",
    "            for synonym in synonyms:\n",
    "                if word != synonym:  # Avoid self-similarity\n",
    "                    similarity_score = get_similarity(word, synonym)\n",
    "                    \n",
    "                    # Include only if the similarity score is above the threshold\n",
    "                    if similarity_score and similarity_score >= threshold:\n",
    "                        synonyms_with_scores[synonym] = similarity_score\n",
    "        \n",
    "        combined_dict[gram] = synonyms_with_scores  # Store the bigram with synonyms and scores\n",
    "    \n",
    "    return combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description Synonyms: {'backend engineer': {}, 'backend engineer responsibility': {'obligation': 1.0, 'duty': 1.0}, 'backend': {}, 'proficiency backend': {}, 'experience database technology': {'engineering': 1.0}, 'data mentor junior': {'wise_man': 1.0, 'Junior': 1.0}, 'data mentor': {'wise_man': 1.0}, 'proficiency backend program': {'plan': 1.0}, 'job description': {'line_of_work': 1.0, 'Job': 1.0, 'occupation': 1.0, 'verbal_description': 1.0}, 'develop maintain': {}, 'development knowledge cloud': {'noesis': 1.0, 'cognition': 1.0}, 'knowledge cloud platform': {'noesis': 1.0, 'cognition': 1.0}, 'backend program': {'plan': 1.0}, 'technology postgresql': {'engineering': 1.0}, 'backend program language': {'plan': 1.0, 'linguistic_communication': 1.0}}\n",
      "##################################################\n",
      "Job Description Synonyms:\n",
      "\n",
      "backend engineer:\n",
      "\n",
      "backend engineer responsibility:\n",
      "  - obligation: 1.0\n",
      "  - duty: 1.0\n",
      "\n",
      "backend:\n",
      "\n",
      "proficiency backend:\n",
      "\n",
      "experience database technology:\n",
      "  - engineering: 1.0\n",
      "\n",
      "data mentor junior:\n",
      "  - wise_man: 1.0\n",
      "  - Junior: 1.0\n",
      "\n",
      "data mentor:\n",
      "  - wise_man: 1.0\n",
      "\n",
      "proficiency backend program:\n",
      "  - plan: 1.0\n",
      "\n",
      "job description:\n",
      "  - line_of_work: 1.0\n",
      "  - Job: 1.0\n",
      "  - occupation: 1.0\n",
      "  - verbal_description: 1.0\n",
      "\n",
      "develop maintain:\n",
      "\n",
      "development knowledge cloud:\n",
      "  - noesis: 1.0\n",
      "  - cognition: 1.0\n",
      "\n",
      "knowledge cloud platform:\n",
      "  - noesis: 1.0\n",
      "  - cognition: 1.0\n",
      "\n",
      "backend program:\n",
      "  - plan: 1.0\n",
      "\n",
      "technology postgresql:\n",
      "  - engineering: 1.0\n",
      "\n",
      "backend program language:\n",
      "  - plan: 1.0\n",
      "  - linguistic_communication: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Combine words and bigrams with synonyms\n",
    "job_synonyms = combine_with_synonyms_and_similarity(key_words_JobD,threshold=0.9)\n",
    "\n",
    "print(f\"Job Description Synonyms: {job_synonyms}\")\n",
    "print('#'*50)\n",
    "# Print the results\n",
    "print(\"Job Description Synonyms:\")\n",
    "for gram, syn_dict in job_synonyms.items():\n",
    "    print(f\"\\n{gram}:\")\n",
    "    for synonym, score in syn_dict.items():\n",
    "        print(f\"  - {synonym}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Synonyms: {'passionate backend development': {}, 'backend development': {}, 'backend development year': {'twelvemonth': 1.0, 'yr': 1.0}, 'project learn experienced': {'task': 1.0, 'undertaking': 1.0, 'larn': 1.0, 'go_through': 1.0}, 'experienced engineer': {'go_through': 1.0}, 'backend': {}, 'asset team': {'plus': 1.0}, 'role egytech': {}, 'learn experienced engineer': {'larn': 1.0, 'go_through': 1.0}, 'development': {}, 'engineer': {}, 'project learn': {'task': 1.0, 'undertaking': 1.0, 'larn': 1.0}, 'nodejs': {}, 'learn experienced': {'larn': 1.0, 'go_through': 1.0}}\n",
      "##################################################\n",
      "interviewee responce Synonyms:\n",
      "\n",
      "passionate backend development:\n",
      "\n",
      "backend development:\n",
      "\n",
      "backend development year:\n",
      "  - twelvemonth: 1.0\n",
      "  - yr: 1.0\n",
      "\n",
      "project learn experienced:\n",
      "  - task: 1.0\n",
      "  - undertaking: 1.0\n",
      "  - larn: 1.0\n",
      "  - go_through: 1.0\n",
      "\n",
      "experienced engineer:\n",
      "  - go_through: 1.0\n",
      "\n",
      "backend:\n",
      "\n",
      "asset team:\n",
      "  - plus: 1.0\n",
      "\n",
      "role egytech:\n",
      "\n",
      "learn experienced engineer:\n",
      "  - larn: 1.0\n",
      "  - go_through: 1.0\n",
      "\n",
      "development:\n",
      "\n",
      "engineer:\n",
      "\n",
      "project learn:\n",
      "  - task: 1.0\n",
      "  - undertaking: 1.0\n",
      "  - larn: 1.0\n",
      "\n",
      "nodejs:\n",
      "\n",
      "learn experienced:\n",
      "  - larn: 1.0\n",
      "  - go_through: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Combine words and bigrams with synonyms\n",
    "response_synonyms = combine_with_synonyms_and_similarity(key_words_interviewee,threshold=0.9)\n",
    "\n",
    "print(f\"Response Synonyms: {response_synonyms}\")\n",
    "\n",
    "print('#'*50)\n",
    "# Print the results\n",
    "print(\"interviewee responce Synonyms:\")\n",
    "for gram, syn_dict in response_synonyms.items():\n",
    "    print(f\"\\n{gram}:\")\n",
    "    for synonym, score in syn_dict.items():\n",
    "        print(f\"  - {synonym}: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Calculate similarity percentage between job description and interviewee responce**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Percentage: 6.67%\n"
     ]
    }
   ],
   "source": [
    "def calculate_similarity(job_synonyms, response_synonyms):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of similarity between job description and interviewee response based on synonyms.\n",
    "\n",
    "    Parameters:\n",
    "        job_synonyms (dict): Dictionary of job description phrases and their synonyms with similarity scores.\n",
    "        response_synonyms (dict): Dictionary of response phrases and their synonyms with similarity scores.\n",
    "\n",
    "    Returns:\n",
    "        float: Percentage of similarity between job description and interviewee response.\n",
    "    \"\"\"\n",
    "    count = 0  # Matches found\n",
    "    total_keywords = len(key_words_JobD)  # Total number of job description keywords\n",
    "\n",
    "    for job_key, job_values in job_synonyms.items():\n",
    "        # Check if the job key or its synonyms exist in the response synonyms\n",
    "        if job_key in response_synonyms:\n",
    "            count += 1\n",
    "        else:\n",
    "            for synonym in job_values.keys():\n",
    "                if synonym in response_synonyms:\n",
    "                    count += 1\n",
    "                    break  # Avoid double counting for the same job key\n",
    "\n",
    "    # Calculate similarity percentage\n",
    "    similarity_percentage = (count / total_keywords) * 100 if total_keywords else 0\n",
    "\n",
    "    return similarity_percentage\n",
    "\n",
    "similarity_percentage = calculate_similarity(job_synonyms, response_synonyms)\n",
    "print(f\"Similarity Percentage: {similarity_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
