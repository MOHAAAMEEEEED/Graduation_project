{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description='''\n",
    "Job Description: Senior Backend Engineer\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "Design, develop, and maintain robust and scalable backend systems.\n",
    "Collaborate with frontend and mobile teams to build seamless user experiences.\n",
    "Optimize database performance and write efficient SQL queries.\n",
    "Implement robust security measures to protect sensitive data.\n",
    "Mentor junior engineers and foster a culture of continuous learning.\n",
    "Required Skills:\n",
    "\n",
    "Strong proficiency in backend programming languages (e.g., Python, Node.js, Ruby on Rails, Java).\n",
    "Experience with database technologies (e.g., PostgreSQL, MySQL, MongoDB).\n",
    "Solid understanding of RESTful API design and development.\n",
    "Knowledge of cloud platforms (e.g., AWS, GCP, Azure).\n",
    "Experience with containerization technologies (e.g., Docker, Kubernetes).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "interviewee_responce='''\n",
    "I've been passionate about backend development for 3 years, and I'm excited to apply my skills to challenging projects.\n",
    "At my previous role at egy_tech, I was responsible for building a scalable API that handled 100 requests per second.\n",
    "I utilized [Specific technologies, e.g., Python, Flask, PostgreSQL] to optimize performance and ensure reliability.\n",
    "\n",
    "I'm particularly interested in your company's focus on [database, data privacy, machine learning, Azuru].\n",
    "I've been exploring Node.js and believe it could be a valuable asset to your team.\n",
    "I'm eager to contribute to innovative projects and learn from experienced engineers.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Mohamed\n",
      "[nltk_data]     Walid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mohamed\n",
      "[nltk_data]     Walid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Mohamed\n",
      "[nltk_data]     Walid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Mohamed\n",
      "[nltk_data]     Walid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Mohamed Walid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "translated_table = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV  # Adverb\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'\\d+', '', text)       # Remove numbers\n",
    "    text = text.translate(translated_table)\n",
    "\n",
    "    text_tokens = word_tokenize(text)\n",
    "\n",
    "    filtered_words=[word for word in text_tokens if word not in stop_words ]\n",
    "    # lemmatization => transforming words to their base or dictionary form\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "    lemma_words = []\n",
    "    for word in filtered_words:\n",
    "        pos_tag = nltk.pos_tag([word])[0][1]  # Get POS tag for each word\n",
    "        wordnet_pos = get_wordnet_pos(pos_tag)  # Map POS to WordNet POS\n",
    "        lemma_word = lemmatizer.lemmatize(word, pos=wordnet_pos)  # Lemmatize using WordNet POS\n",
    "        lemma_words.append(lemma_word)\n",
    "\n",
    "    processed_text = ' '.join(lemma_words)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed job description : job description senior backend engineer responsibility design develop maintain robust scalable backend system collaborate frontend mobile team build seamless user experience optimize database performance write efficient sql query implement robust security measure protect sensitive data mentor junior engineer foster culture continuous learn require skill strong proficiency backend program language eg python nodejs ruby rail java experience database technology eg postgresql mysql mongodb solid understand restful api design development knowledge cloud platform eg aws gcp azure experience containerization technology eg docker kubernetes\n"
     ]
    }
   ],
   "source": [
    "preprocessed_job_description = preprocess_text(job_description)\n",
    "print(f\"Preprocessed job description : {preprocessed_job_description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed interviewee responce : ive passionate backend development year im excite apply skill challenge project previous role egytech responsible building scalable api handle request per second utilized specific technology eg python flask postgresql optimize performance ensure reliability im particularly interested company focus database data privacy machine learn azuru ive explore nodejs believe could valuable asset team im eager contribute innovative project learn experienced engineer\n"
     ]
    }
   ],
   "source": [
    "preprocessed_interviewee_responce= preprocess_text(interviewee_responce)\n",
    "print(f\"Preprocessed interviewee responce : {preprocessed_interviewee_responce}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Keyword-based-similarity***\n",
    "##### LEXIACAL SIMILARITY:      TF-IDF, cosine similarity, Jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***TF-IDF***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[preprocessed_job_description,preprocessed_interviewee_responce]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "tf_idf=TfidfVectorizer()\n",
    "sparse_matrix=tf_idf.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cosine Similarity Score using TF-IDF:  0.18\n",
      "\n",
      " Matching Words:  ['api', 'backend', 'data', 'database', 'development', 'eg', 'engineer', 'learn', 'nodejs', 'optimize', 'performance', 'postgresql', 'python', 'scalable', 'skill', 'team', 'technology']\n"
     ]
    }
   ],
   "source": [
    "# compute cosine similarity between all pairs of documents\n",
    "\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df=pd.DataFrame(doc_term_matrix, columns=tf_idf.get_feature_names_out(), index=['job_description','interviewee_responce'])\n",
    "\n",
    "similarity_scores = cosine_similarity(df,df)[0,1]\n",
    "match_keys=df.isin([0]).sum(axis=0)\n",
    "match_words=match_keys[match_keys==0].keys()\n",
    "\n",
    "\n",
    "# print the similarity score\n",
    "print(\"\\n Cosine Similarity Score using TF-IDF: \", round(similarity_scores,2))\n",
    "print(\"\\n Matching Words: \", list(match_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***jaccard***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Lexical similarity methods:\n",
      "Jaccard:  0.6907692307692308\n",
      "Sorensen:  0.82\n"
     ]
    }
   ],
   "source": [
    "import textdistance as td\n",
    "\n",
    "print(\"\\n Lexical similarity methods:\")\n",
    "print(\"Jaccard: \", td.jaccard.similarity(preprocessed_job_description, preprocessed_interviewee_responce))\n",
    "print(\"Sorensen: \", round(td.sorensen_dice.similarity(preprocessed_job_description, preprocessed_interviewee_responce),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP techniques\n",
    "####  capture semantic meaning, context, and relationships between words or phrases, thereby providing more accurate and nuanced measures of text similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Base uncased  using a masked language modeling (MLM) objective\n",
    "#### This model is uncased which means it does not make a difference between upper or lowercase, for example, \"english\" = \"English\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-based Similarity Score (util.cos_sim): 0.76\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained model \n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')        \n",
    "\n",
    "# Generate embeddings\n",
    "job_description_embedding = model.encode(preprocessed_job_description, convert_to_tensor=True)\n",
    "interviewee_response_embedding = model.encode(preprocessed_interviewee_responce, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity using util.cos_sim\n",
    "similarity_score = util.cos_sim(job_description_embedding, interviewee_response_embedding)\n",
    "print(f\"Context-based Similarity Score (util.cos_sim): {similarity_score.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  DistilBERT model \n",
    "#### small, fast, cheap and light Transformer model trained by distilling BERT base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score (util.cos_sim): 0.82\n"
     ]
    }
   ],
   "source": [
    "distil_model=SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "# Generate embeddings\n",
    "job_description_embeddingg = distil_model.encode(preprocessed_job_description, convert_to_tensor=True)\n",
    "interviewee_response_embeddingg = distil_model.encode(preprocessed_interviewee_responce, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity using util.cos_sim\n",
    "similarity_score = util.cos_sim(job_description_embeddingg, interviewee_response_embeddingg)\n",
    "print(f\"Similarity Score (util.cos_sim): {similarity_score.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
